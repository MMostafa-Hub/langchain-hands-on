{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAIChat\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.router.llm_router import RouterOutputParser, LLMRouterChain\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains import (\n",
    "    LLMChain,\n",
    "    SimpleSequentialChain,\n",
    "    SequentialChain,\n",
    "    MultiPromptChain,\n",
    ")\n",
    "from langchain.globals import set_debug\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "set_debug(True)  # Enable debug mode\n",
    "load_dotenv()  # Load the variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain is an encapsulation of Model, prompt, and optionally a parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/llms/openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIChat(temperature=0.9)\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"product\": \"Potato Chips\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"What is the best name to describe     a company that makes Potato Chips?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OpenAIChat] [711ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Crunch Factory\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 22,\n",
      "      \"completion_tokens\": 3,\n",
      "      \"total_tokens\": 25\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [758ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The Crunch Factory\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product': 'Potato Chips', 'text': 'The Crunch Factory'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"Potato Chips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains can be connected to each other. The output of the previous chain is the input of the next chain. The `SimpleSequentialChain` is a simple chain that can be used to connect multiple chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simpler method to create a template.\n",
    "prompt_1 = PromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(llm=model, prompt=prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = PromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "\n",
    "chain_2 = LLMChain(llm=model, prompt=prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chain = SimpleSequentialChain(chains=[chain_1, chain_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Potato Chips\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"product\": \"Potato Chips\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"What is the best name to describe     a company that makes Potato Chips?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain > 3:llm:OpenAIChat] [1.90s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\"Crisp Delights\\\"\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 22,\n",
      "      \"completion_tokens\": 6,\n",
      "      \"total_tokens\": 28\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 2:chain:LLMChain] [1.91s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\\"Crisp Delights\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"company_name\": \"\\\"Crisp Delights\\\"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Write a 20 words description for the following     company:\\\"Crisp Delights\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain > 5:llm:OpenAIChat] [1.72s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Crisp Delights offers a tantalizing selection of gourmet snacks and treats that are full of flavor and satisfaction.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 25,\n",
      "      \"completion_tokens\": 23,\n",
      "      \"total_tokens\": 48\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain > 4:chain:LLMChain] [1.72s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Crisp Delights offers a tantalizing selection of gourmet snacks and treats that are full of flavor and satisfaction.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SimpleSequentialChain] [3.68s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Crisp Delights offers a tantalizing selection of gourmet snacks and treats that are full of flavor and satisfaction.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Potato Chips',\n",
       " 'output': 'Crisp Delights offers a tantalizing selection of gourmet snacks and treats that are full of flavor and satisfaction.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_chain(\"Potato Chips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it can be used to connect multiple chains non-sequentially, by specifying `output_key` that the next chain needs to receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1: translate to english\n",
    "prompt_1 = PromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\" \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input = Review and output = English_Review\n",
    "chain_1 = LLMChain(llm=model, prompt=prompt_1, output_key=\"English_Review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2: summarize\n",
    "prompt_2 = PromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\" \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input = English_Review and output = summary\n",
    "chain_2 = LLMChain(llm=model, prompt=prompt_2, output_key=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: Know the language\n",
    "prompt_3 = PromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input = Review and output = language\n",
    "chain_3 = LLMChain(llm=model, prompt=prompt_3, output_key=\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "prompt_4 = PromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \\\n",
    "    summary in the specified language: \\\n",
    "    \\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input = summary, language and output = followup_message\n",
    "chain_4 = LLMChain(llm=model, prompt=prompt_4, output_key=\"followup_message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential_chain: input = Review\n",
    "# and output = English_Review,summary, followup_message\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_customer_review = \"\"\"\\\n",
    "Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. \\\n",
    "J'achète les mêmes dans le commerce et le goût est bien meilleur.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur.\\\"\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur.\\\"\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain > 3:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Translate the following review to english:\\n\\nJe trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur.\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain > 3:llm:OpenAIChat] [2.31s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 54,\n",
      "      \"completion_tokens\": 30,\n",
      "      \"total_tokens\": 84\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain] [2.31s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"English_Review\": \"I find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur.\\\"\\n\",\n",
      "  \"English_Review\": \"I find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain > 5:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Can you summarize the following review in 1 sentence:\\n\\nI find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain > 5:llm:OpenAIChat] [1.95s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The reviewer was disappointed with the mediocre taste and lack of foam stability, contrasting it with a better taste experience from the same product purchased in stores.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 48,\n",
      "      \"completion_tokens\": 29,\n",
      "      \"total_tokens\": 77\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain] [1.96s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"The reviewer was disappointed with the mediocre taste and lack of foam stability, contrasting it with a better taste experience from the same product purchased in stores.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur.\\\"\\n\",\n",
      "  \"English_Review\": \"I find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\",\n",
      "  \"summary\": \"The reviewer was disappointed with the mediocre taste and lack of foam stability, contrasting it with a better taste experience from the same product purchased in stores.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain > 7:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"What language is the following review:\\n\\nJe trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur.\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain > 7:llm:OpenAIChat] [1.07s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The review is in French.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 54,\n",
      "      \"completion_tokens\": 6,\n",
      "      \"total_tokens\": 60\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain] [1.07s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"language\": \"The review is in French.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 8:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"Review\": \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur.\\\"\\n\",\n",
      "  \"English_Review\": \"I find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\",\n",
      "  \"summary\": \"The reviewer was disappointed with the mediocre taste and lack of foam stability, contrasting it with a better taste experience from the same product purchased in stores.\",\n",
      "  \"language\": \"The review is in French.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 8:chain:LLMChain > 9:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Write a follow up response to the following     summary in the specified language:     \\n\\nSummary: The reviewer was disappointed with the mediocre taste and lack of foam stability, contrasting it with a better taste experience from the same product purchased in stores.\\n\\nLanguage: The review is in French.\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 8:chain:LLMChain > 9:llm:OpenAIChat] [36.56s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Réponse de suivi:\\n\\nCher(e) critique,\\n\\nNous vous remercions d'avoir partagé vos commentaires sur votre expérience d'achat de notre produit. Nous sommes désolés d'apprendre que vous avez été déçu(e) par le goût médiocre et la faible stabilité de la mousse. Votre opinion est importante pour nous, et nous souhaitons nous améliorer en conséquence.\\n\\nNous tenons à souligner que nous prenons toutes les mesures nécessaires pour garantir la qualité de nos produits, y compris leur goût et leur texture. Cependant, il peut arriver que des variations se produisent en fonction de plusieurs facteurs, tels que la manipulation ou le stockage du produit.\\n\\nNous aimerions mieux comprendre votre expérience et identifier la cause de ce problème. Pourriez-vous nous fournir plus de détails, tels que le numéro de lot ou la date d'achat, afin que nous puissions enquêter plus en profondeur?\\n\\nEn attendant, nous vous recommandons de retourner le produit à l'endroit où vous l'avez acheté, en présentant votre ticket de caisse, afin que vous puissiez obtenir un remboursement ou un échange. Nous voulons nous assurer que vous soyez entièrement satisfait(e) de votre achat.\\n\\nNous espérons que vous nous donnerez une autre chance de vous impressionner avec nos produits à l'avenir. Votre satisfaction est notre priorité absolue.\\n\\nBien à vous,\\n\\nL'équipe du service client.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 62,\n",
      "      \"completion_tokens\": 343,\n",
      "      \"total_tokens\": 405\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 8:chain:LLMChain] [36.57s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"followup_message\": \"Réponse de suivi:\\n\\nCher(e) critique,\\n\\nNous vous remercions d'avoir partagé vos commentaires sur votre expérience d'achat de notre produit. Nous sommes désolés d'apprendre que vous avez été déçu(e) par le goût médiocre et la faible stabilité de la mousse. Votre opinion est importante pour nous, et nous souhaitons nous améliorer en conséquence.\\n\\nNous tenons à souligner que nous prenons toutes les mesures nécessaires pour garantir la qualité de nos produits, y compris leur goût et leur texture. Cependant, il peut arriver que des variations se produisent en fonction de plusieurs facteurs, tels que la manipulation ou le stockage du produit.\\n\\nNous aimerions mieux comprendre votre expérience et identifier la cause de ce problème. Pourriez-vous nous fournir plus de détails, tels que le numéro de lot ou la date d'achat, afin que nous puissions enquêter plus en profondeur?\\n\\nEn attendant, nous vous recommandons de retourner le produit à l'endroit où vous l'avez acheté, en présentant votre ticket de caisse, afin que vous puissiez obtenir un remboursement ou un échange. Nous voulons nous assurer que vous soyez entièrement satisfait(e) de votre achat.\\n\\nNous espérons que vous nous donnerez une autre chance de vous impressionner avec nos produits à l'avenir. Votre satisfaction est notre priorité absolue.\\n\\nBien à vous,\\n\\nL'équipe du service client.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain] [41.91s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"English_Review\": \"I find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\",\n",
      "  \"summary\": \"The reviewer was disappointed with the mediocre taste and lack of foam stability, contrasting it with a better taste experience from the same product purchased in stores.\",\n",
      "  \"followup_message\": \"Réponse de suivi:\\n\\nCher(e) critique,\\n\\nNous vous remercions d'avoir partagé vos commentaires sur votre expérience d'achat de notre produit. Nous sommes désolés d'apprendre que vous avez été déçu(e) par le goût médiocre et la faible stabilité de la mousse. Votre opinion est importante pour nous, et nous souhaitons nous améliorer en conséquence.\\n\\nNous tenons à souligner que nous prenons toutes les mesures nécessaires pour garantir la qualité de nos produits, y compris leur goût et leur texture. Cependant, il peut arriver que des variations se produisent en fonction de plusieurs facteurs, tels que la manipulation ou le stockage du produit.\\n\\nNous aimerions mieux comprendre votre expérience et identifier la cause de ce problème. Pourriez-vous nous fournir plus de détails, tels que le numéro de lot ou la date d'achat, afin que nous puissions enquêter plus en profondeur?\\n\\nEn attendant, nous vous recommandons de retourner le produit à l'endroit où vous l'avez acheté, en présentant votre ticket de caisse, afin que vous puissiez obtenir un remboursement ou un échange. Nous voulons nous assurer que vous soyez entièrement satisfait(e) de votre achat.\\n\\nNous espérons que vous nous donnerez une autre chance de vous impressionner avec nos produits à l'avenir. Votre satisfaction est notre priorité absolue.\\n\\nBien à vous,\\n\\nL'équipe du service client.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sequential_response = sequential_chain(french_customer_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review': 'Je trouve le goût médiocre. La mousse ne tient pas, c\\'est bizarre. J\\'achète les mêmes dans le commerce et le goût est bien meilleur.\"\\n',\n",
       " 'English_Review': \"I find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in stores and the taste is much better.\",\n",
       " 'summary': 'The reviewer was disappointed with the mediocre taste and lack of foam stability, contrasting it with a better taste experience from the same product purchased in stores.',\n",
       " 'followup_message': \"Réponse de suivi:\\n\\nCher(e) critique,\\n\\nNous vous remercions d'avoir partagé vos commentaires sur votre expérience d'achat de notre produit. Nous sommes désolés d'apprendre que vous avez été déçu(e) par le goût médiocre et la faible stabilité de la mousse. Votre opinion est importante pour nous, et nous souhaitons nous améliorer en conséquence.\\n\\nNous tenons à souligner que nous prenons toutes les mesures nécessaires pour garantir la qualité de nos produits, y compris leur goût et leur texture. Cependant, il peut arriver que des variations se produisent en fonction de plusieurs facteurs, tels que la manipulation ou le stockage du produit.\\n\\nNous aimerions mieux comprendre votre expérience et identifier la cause de ce problème. Pourriez-vous nous fournir plus de détails, tels que le numéro de lot ou la date d'achat, afin que nous puissions enquêter plus en profondeur?\\n\\nEn attendant, nous vous recommandons de retourner le produit à l'endroit où vous l'avez acheté, en présentant votre ticket de caisse, afin que vous puissiez obtenir un remboursement ou un échange. Nous voulons nous assurer que vous soyez entièrement satisfait(e) de votre achat.\\n\\nNous espérons que vous nous donnerez une autre chance de vous impressionner avec nos produits à l'avenir. Votre satisfaction est notre priorité absolue.\\n\\nBien à vous,\\n\\nL'équipe du service client.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RouterChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RouterChain` is a chain that can be used to route the input to different chains based on the input key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics\",\n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\",\n",
    "        \"description\": \"Good for answering history questions\",\n",
    "        \"prompt_template\": history_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\",\n",
    "        \"description\": \"Good for answering computer science questions\",\n",
    "        \"prompt_template\": computerscience_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/llms/openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIChat(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the destination chains\n",
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=model, prompt=prompt)\n",
    "    destination_chains[name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a default chain that will be used when the router is not sure which chain to use\n",
    "default_prompt = PromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=model, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the destination choices string\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prompt template that will be used to route the input to the correct destination\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The router that decides which destination chain to use\n",
    "router_chain = LLMRouterChain.from_llm(llm=model, prompt=router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_prompt_chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    default_chain=default_chain,\n",
    "    destination_chains=destination_chains,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the speed of light?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the speed of light?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the speed of light?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{\\n    \\\"destination\\\": string \\\\ name of the prompt to use or \\\"DEFAULT\\\"\\n    \\\"next_inputs\\\": string \\\\ a potentially modified version of the original input\\n}\\n```\\n\\nREMEMBER: \\\"destination\\\" MUST be one of the candidate prompt names specified below OR it can be \\\"DEFAULT\\\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \\\"next_inputs\\\" can just be the original input if you don't think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions\\n\\n<< INPUT >>\\nWhat is the speed of light?\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] [2.10s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"physics\\\",\\n    \\\"next_inputs\\\": \\\"What is the speed of light?\\\"\\n}\\n```\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 276,\n",
      "      \"completion_tokens\": 26,\n",
      "      \"total_tokens\": 302\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] [2.10s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"physics\\\",\\n    \\\"next_inputs\\\": \\\"What is the speed of light?\\\"\\n}\\n```\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] [2.11s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"destination\": \"physics\",\n",
      "  \"next_inputs\": {\n",
      "    \"input\": \"What is the speed of light?\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the speed of light?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a very smart physics professor. You are great at answering questions about physics in a conciseand easy to understand manner. When you don't know the answer to a question you admitthat you don't know.\\n\\nHere is a question:\\nWhat is the speed of light?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] [1.84s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The speed of light in a vacuum is approximately 299,792,458 meters per second, or about 186,282 miles per second.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 62,\n",
      "      \"completion_tokens\": 29,\n",
      "      \"total_tokens\": 91\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] [1.84s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The speed of light in a vacuum is approximately 299,792,458 meters per second, or about 186,282 miles per second.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] [3.95s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the speed of light?\",\n",
      "  \"text\": \"The speed of light in a vacuum is approximately 299,792,458 meters per second, or about 186,282 miles per second.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the speed of light?',\n",
       " 'text': 'The speed of light in a vacuum is approximately 299,792,458 meters per second, or about 186,282 miles per second.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"What is the speed of light?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see from the logs that the input is routed to the `physics` chain as the input topic is physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is 1 + 1?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is 1 + 1?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is 1 + 1?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{\\n    \\\"destination\\\": string \\\\ name of the prompt to use or \\\"DEFAULT\\\"\\n    \\\"next_inputs\\\": string \\\\ a potentially modified version of the original input\\n}\\n```\\n\\nREMEMBER: \\\"destination\\\" MUST be one of the candidate prompt names specified below OR it can be \\\"DEFAULT\\\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \\\"next_inputs\\\" can just be the original input if you don't think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions\\n\\n<< INPUT >>\\nWhat is 1 + 1?\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] [2.39s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"math\\\",\\n    \\\"next_inputs\\\": \\\"What is 1 + 1?\\\"\\n}\\n```\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 277,\n",
      "      \"completion_tokens\": 27,\n",
      "      \"total_tokens\": 304\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] [2.39s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"math\\\",\\n    \\\"next_inputs\\\": \\\"What is 1 + 1?\\\"\\n}\\n```\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] [2.39s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"destination\": \"math\",\n",
      "  \"next_inputs\": {\n",
      "    \"input\": \"What is 1 + 1?\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is 1 + 1?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, \\nanswer the component parts, and then put them togetherto answer the broader question.\\n\\nHere is a question:\\nWhat is 1 + 1?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] [3.54s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thank you for your kind words! The question \\\"What is 1 + 1?\\\" is a simple addition problem. To solve it, we add the numbers 1 and 1 together. \\n\\n1 + 1 = 2\\n\\nTherefore, the answer to the question \\\"What is 1 + 1?\\\" is 2.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 73,\n",
      "      \"completion_tokens\": 68,\n",
      "      \"total_tokens\": 141\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] [3.54s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Thank you for your kind words! The question \\\"What is 1 + 1?\\\" is a simple addition problem. To solve it, we add the numbers 1 and 1 together. \\n\\n1 + 1 = 2\\n\\nTherefore, the answer to the question \\\"What is 1 + 1?\\\" is 2.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] [5.94s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is 1 + 1?\",\n",
      "  \"text\": \"Thank you for your kind words! The question \\\"What is 1 + 1?\\\" is a simple addition problem. To solve it, we add the numbers 1 and 1 together. \\n\\n1 + 1 = 2\\n\\nTherefore, the answer to the question \\\"What is 1 + 1?\\\" is 2.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 1 + 1?',\n",
       " 'text': 'Thank you for your kind words! The question \"What is 1 + 1?\" is a simple addition problem. To solve it, we add the numbers 1 and 1 together. \\n\\n1 + 1 = 2\\n\\nTherefore, the answer to the question \"What is 1 + 1?\" is 2.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Who was the first president of the United States?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Who was the first president of the United States?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Who was the first president of the United States?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{\\n    \\\"destination\\\": string \\\\ name of the prompt to use or \\\"DEFAULT\\\"\\n    \\\"next_inputs\\\": string \\\\ a potentially modified version of the original input\\n}\\n```\\n\\nREMEMBER: \\\"destination\\\" MUST be one of the candidate prompt names specified below OR it can be \\\"DEFAULT\\\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \\\"next_inputs\\\" can just be the original input if you don't think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions\\n\\n<< INPUT >>\\nWho was the first president of the United States?\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] [2.95s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"History\\\",\\n    \\\"next_inputs\\\": \\\"Who was the first president of the United States?\\\"\\n}\\n```\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 279,\n",
      "      \"completion_tokens\": 29,\n",
      "      \"total_tokens\": 308\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] [2.95s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"History\\\",\\n    \\\"next_inputs\\\": \\\"Who was the first president of the United States?\\\"\\n}\\n```\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] [2.95s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"destination\": \"History\",\n",
      "  \"next_inputs\": {\n",
      "    \"input\": \"Who was the first president of the United States?\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Who was the first president of the United States?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a very good historian. You have an excellent knowledge of and understanding of people,events and contexts from a range of historical periods. You have the ability to think, reflect, debate, discuss and evaluate the past. You have a respect for historical evidenceand the ability to make use of it to support your explanations and judgements.\\n\\nHere is a question:\\nWho was the first president of the United States?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] [13.43s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The first president of the United States was George Washington. He served as the president from 1789 to 1797. Washington played a crucial role in the formation of the United States and is often referred to as the \\\"Father of His Country.\\\" He was a key figure in the American Revolutionary War and was unanimously elected as the first president under the newly established U.S. Constitution. Washington's leadership and contributions to the early years of the nation set important precedents for future presidents and helped shape the office of the presidency.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 90,\n",
      "      \"completion_tokens\": 105,\n",
      "      \"total_tokens\": 195\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] [13.43s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The first president of the United States was George Washington. He served as the president from 1789 to 1797. Washington played a crucial role in the formation of the United States and is often referred to as the \\\"Father of His Country.\\\" He was a key figure in the American Revolutionary War and was unanimously elected as the first president under the newly established U.S. Constitution. Washington's leadership and contributions to the early years of the nation set important precedents for future presidents and helped shape the office of the presidency.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] [16.39s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Who was the first president of the United States?\",\n",
      "  \"text\": \"The first president of the United States was George Washington. He served as the president from 1789 to 1797. Washington played a crucial role in the formation of the United States and is often referred to as the \\\"Father of His Country.\\\" He was a key figure in the American Revolutionary War and was unanimously elected as the first president under the newly established U.S. Constitution. Washington's leadership and contributions to the early years of the nation set important precedents for future presidents and helped shape the office of the presidency.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who was the first president of the United States?',\n",
       " 'text': 'The first president of the United States was George Washington. He served as the president from 1789 to 1797. Washington played a crucial role in the formation of the United States and is often referred to as the \"Father of His Country.\" He was a key figure in the American Revolutionary War and was unanimously elected as the first president under the newly established U.S. Constitution. Washington\\'s leadership and contributions to the early years of the nation set important precedents for future presidents and helped shape the office of the presidency.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"Who was the first president of the United States?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"explain the quicksort algorithm\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"explain the quicksort algorithm\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"explain the quicksort algorithm\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{\\n    \\\"destination\\\": string \\\\ name of the prompt to use or \\\"DEFAULT\\\"\\n    \\\"next_inputs\\\": string \\\\ a potentially modified version of the original input\\n}\\n```\\n\\nREMEMBER: \\\"destination\\\" MUST be one of the candidate prompt names specified below OR it can be \\\"DEFAULT\\\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \\\"next_inputs\\\" can just be the original input if you don't think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions\\n\\n<< INPUT >>\\nexplain the quicksort algorithm\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] [17.58s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"computer science\\\",\\n    \\\"next_inputs\\\": \\\"explain the quicksort algorithm\\\"\\n}\\n```\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 275,\n",
      "      \"completion_tokens\": 26,\n",
      "      \"total_tokens\": 301\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] [17.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"computer science\\\",\\n    \\\"next_inputs\\\": \\\"explain the quicksort algorithm\\\"\\n}\\n```\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] [17.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"destination\": \"computer science\",\n",
      "  \"next_inputs\": {\n",
      "    \"input\": \"explain the quicksort algorithm\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"explain the quicksort algorithm\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are a successful computer scientist.You have a passion for creativity, collaboration,forward-thinking, confidence, strong problem-solving capabilities,understanding of theories and algorithms, and excellent communication skills. You are great at answering coding questions. You are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity. \\n\\nHere is a question:\\nexplain the quicksort algorithm\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] [37.82s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The quicksort algorithm is a widely used sorting algorithm that follows the divide-and-conquer approach. It efficiently sorts an array or list by partitioning it into smaller sub-arrays, sorting those sub-arrays recursively, and then combining them to obtain a sorted array.\\n\\nHere is a step-by-step explanation of the quicksort algorithm:\\n\\n1. Choose a pivot element from the array. The pivot can be any element, but commonly the first or last element is chosen.\\n\\n2. Partition the array into two sub-arrays: elements less than the pivot and elements greater than the pivot. This is done by iterating through the array and comparing each element to the pivot. If an element is smaller than the pivot, it is moved to the left sub-array, otherwise to the right sub-array.\\n\\n3. Recursively apply steps 1 and 2 to the sub-arrays created in the previous step. This means choosing a pivot for each sub-array and partitioning them further until the sub-arrays contain only one element or are empty.\\n\\n4. Combine the sorted sub-arrays obtained from the recursive steps. This can be done by concatenating the sorted left sub-array, the pivot element, and the sorted right sub-array.\\n\\n5. The array is now sorted.\\n\\nThe key idea behind quicksort is the partitioning step, which efficiently places the pivot element in its correct position while dividing the array into two smaller sub-arrays. This process is repeated recursively until the entire array is sorted.\\n\\nQuicksort has an average time complexity of O(n log n), where n is the number of elements in the array. However, in the worst-case scenario, when the pivot is consistently chosen as the smallest or largest element, the time complexity can degrade to O(n^2). To mitigate this, various techniques like choosing a random pivot or using the median-of-three method can be employed.\\n\\nOverall, quicksort is a highly efficient and widely used sorting algorithm due to its simplicity, effectiveness, and average-case time complexity.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 108,\n",
      "      \"completion_tokens\": 402,\n",
      "      \"total_tokens\": 510\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] [37.83s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The quicksort algorithm is a widely used sorting algorithm that follows the divide-and-conquer approach. It efficiently sorts an array or list by partitioning it into smaller sub-arrays, sorting those sub-arrays recursively, and then combining them to obtain a sorted array.\\n\\nHere is a step-by-step explanation of the quicksort algorithm:\\n\\n1. Choose a pivot element from the array. The pivot can be any element, but commonly the first or last element is chosen.\\n\\n2. Partition the array into two sub-arrays: elements less than the pivot and elements greater than the pivot. This is done by iterating through the array and comparing each element to the pivot. If an element is smaller than the pivot, it is moved to the left sub-array, otherwise to the right sub-array.\\n\\n3. Recursively apply steps 1 and 2 to the sub-arrays created in the previous step. This means choosing a pivot for each sub-array and partitioning them further until the sub-arrays contain only one element or are empty.\\n\\n4. Combine the sorted sub-arrays obtained from the recursive steps. This can be done by concatenating the sorted left sub-array, the pivot element, and the sorted right sub-array.\\n\\n5. The array is now sorted.\\n\\nThe key idea behind quicksort is the partitioning step, which efficiently places the pivot element in its correct position while dividing the array into two smaller sub-arrays. This process is repeated recursively until the entire array is sorted.\\n\\nQuicksort has an average time complexity of O(n log n), where n is the number of elements in the array. However, in the worst-case scenario, when the pivot is consistently chosen as the smallest or largest element, the time complexity can degrade to O(n^2). To mitigate this, various techniques like choosing a random pivot or using the median-of-three method can be employed.\\n\\nOverall, quicksort is a highly efficient and widely used sorting algorithm due to its simplicity, effectiveness, and average-case time complexity.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] [55.41s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"explain the quicksort algorithm\",\n",
      "  \"text\": \"The quicksort algorithm is a widely used sorting algorithm that follows the divide-and-conquer approach. It efficiently sorts an array or list by partitioning it into smaller sub-arrays, sorting those sub-arrays recursively, and then combining them to obtain a sorted array.\\n\\nHere is a step-by-step explanation of the quicksort algorithm:\\n\\n1. Choose a pivot element from the array. The pivot can be any element, but commonly the first or last element is chosen.\\n\\n2. Partition the array into two sub-arrays: elements less than the pivot and elements greater than the pivot. This is done by iterating through the array and comparing each element to the pivot. If an element is smaller than the pivot, it is moved to the left sub-array, otherwise to the right sub-array.\\n\\n3. Recursively apply steps 1 and 2 to the sub-arrays created in the previous step. This means choosing a pivot for each sub-array and partitioning them further until the sub-arrays contain only one element or are empty.\\n\\n4. Combine the sorted sub-arrays obtained from the recursive steps. This can be done by concatenating the sorted left sub-array, the pivot element, and the sorted right sub-array.\\n\\n5. The array is now sorted.\\n\\nThe key idea behind quicksort is the partitioning step, which efficiently places the pivot element in its correct position while dividing the array into two smaller sub-arrays. This process is repeated recursively until the entire array is sorted.\\n\\nQuicksort has an average time complexity of O(n log n), where n is the number of elements in the array. However, in the worst-case scenario, when the pivot is consistently chosen as the smallest or largest element, the time complexity can degrade to O(n^2). To mitigate this, various techniques like choosing a random pivot or using the median-of-three method can be employed.\\n\\nOverall, quicksort is a highly efficient and widely used sorting algorithm due to its simplicity, effectiveness, and average-case time complexity.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'explain the quicksort algorithm',\n",
       " 'text': 'The quicksort algorithm is a widely used sorting algorithm that follows the divide-and-conquer approach. It efficiently sorts an array or list by partitioning it into smaller sub-arrays, sorting those sub-arrays recursively, and then combining them to obtain a sorted array.\\n\\nHere is a step-by-step explanation of the quicksort algorithm:\\n\\n1. Choose a pivot element from the array. The pivot can be any element, but commonly the first or last element is chosen.\\n\\n2. Partition the array into two sub-arrays: elements less than the pivot and elements greater than the pivot. This is done by iterating through the array and comparing each element to the pivot. If an element is smaller than the pivot, it is moved to the left sub-array, otherwise to the right sub-array.\\n\\n3. Recursively apply steps 1 and 2 to the sub-arrays created in the previous step. This means choosing a pivot for each sub-array and partitioning them further until the sub-arrays contain only one element or are empty.\\n\\n4. Combine the sorted sub-arrays obtained from the recursive steps. This can be done by concatenating the sorted left sub-array, the pivot element, and the sorted right sub-array.\\n\\n5. The array is now sorted.\\n\\nThe key idea behind quicksort is the partitioning step, which efficiently places the pivot element in its correct position while dividing the array into two smaller sub-arrays. This process is repeated recursively until the entire array is sorted.\\n\\nQuicksort has an average time complexity of O(n log n), where n is the number of elements in the array. However, in the worst-case scenario, when the pivot is consistently chosen as the smallest or largest element, the time complexity can degrade to O(n^2). To mitigate this, various techniques like choosing a random pivot or using the median-of-three method can be employed.\\n\\nOverall, quicksort is a highly efficient and widely used sorting algorithm due to its simplicity, effectiveness, and average-case time complexity.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"explain the quicksort algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the functions of the heart?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the functions of the heart?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the functions of the heart?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{\\n    \\\"destination\\\": string \\\\ name of the prompt to use or \\\"DEFAULT\\\"\\n    \\\"next_inputs\\\": string \\\\ a potentially modified version of the original input\\n}\\n```\\n\\nREMEMBER: \\\"destination\\\" MUST be one of the candidate prompt names specified below OR it can be \\\"DEFAULT\\\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \\\"next_inputs\\\" can just be the original input if you don't think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions\\n\\n<< INPUT >>\\nWhat is the functions of the heart?\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain > 4:llm:OpenAIChat] [1.52s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"DEFAULT\\\",\\n    \\\"next_inputs\\\": \\\"What is the functions of the heart?\\\"\\n}\\n```\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 277,\n",
      "      \"completion_tokens\": 27,\n",
      "      \"total_tokens\": 304\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain > 3:chain:LLMChain] [1.52s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"```json\\n{\\n    \\\"destination\\\": \\\"DEFAULT\\\",\\n    \\\"next_inputs\\\": \\\"What is the functions of the heart?\\\"\\n}\\n```\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 2:chain:LLMRouterChain] [1.52s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"destination\": null,\n",
      "  \"next_inputs\": {\n",
      "    \"input\": \"What is the functions of the heart?\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the functions of the heart?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"What is the functions of the heart?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain > 6:llm:OpenAIChat] [22.51s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The heart is a vital organ that performs several important functions in the body. Some of the main functions of the heart include:\\n\\n1. Pumping blood: The heart acts as a muscular pump that continuously circulates blood throughout the body. It receives oxygenated blood from the lungs and pumps it to the rest of the body, delivering oxygen and nutrients to the cells and organs.\\n\\n2. Oxygenation: The heart receives deoxygenated blood from the body and pumps it to the lungs, where it picks up oxygen and gets rid of carbon dioxide. This oxygenation process ensures that the body's cells receive the necessary oxygen for their proper functioning.\\n\\n3. Regulation of blood pressure: The heart helps regulate blood pressure by adjusting the force and rate of its contractions. It pumps blood with enough force to maintain adequate blood flow and pressure throughout the body.\\n\\n4. Circulatory system support: The heart is the central organ of the circulatory system, which includes blood vessels, arteries, veins, and capillaries. It ensures the continuous flow of blood, allowing nutrients, hormones, and other substances to be transported to various parts of the body.\\n\\n5. Removal of waste products: The heart helps remove waste products, such as carbon dioxide and metabolic byproducts, from the body by pumping deoxygenated blood to the lungs for oxygenation and elimination of waste gases.\\n\\n6. Hormone transportation: The heart transports hormones and other signaling molecules throughout the body, helping to regulate various physiological processes.\\n\\nOverall, the heart plays a crucial role in maintaining the body's overall health and functioning by ensuring the continuous circulation of oxygen, nutrients, and waste products.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 15,\n",
      "      \"completion_tokens\": 330,\n",
      "      \"total_tokens\": 345\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain > 5:chain:LLMChain] [22.51s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The heart is a vital organ that performs several important functions in the body. Some of the main functions of the heart include:\\n\\n1. Pumping blood: The heart acts as a muscular pump that continuously circulates blood throughout the body. It receives oxygenated blood from the lungs and pumps it to the rest of the body, delivering oxygen and nutrients to the cells and organs.\\n\\n2. Oxygenation: The heart receives deoxygenated blood from the body and pumps it to the lungs, where it picks up oxygen and gets rid of carbon dioxide. This oxygenation process ensures that the body's cells receive the necessary oxygen for their proper functioning.\\n\\n3. Regulation of blood pressure: The heart helps regulate blood pressure by adjusting the force and rate of its contractions. It pumps blood with enough force to maintain adequate blood flow and pressure throughout the body.\\n\\n4. Circulatory system support: The heart is the central organ of the circulatory system, which includes blood vessels, arteries, veins, and capillaries. It ensures the continuous flow of blood, allowing nutrients, hormones, and other substances to be transported to various parts of the body.\\n\\n5. Removal of waste products: The heart helps remove waste products, such as carbon dioxide and metabolic byproducts, from the body by pumping deoxygenated blood to the lungs for oxygenation and elimination of waste gases.\\n\\n6. Hormone transportation: The heart transports hormones and other signaling molecules throughout the body, helping to regulate various physiological processes.\\n\\nOverall, the heart plays a crucial role in maintaining the body's overall health and functioning by ensuring the continuous circulation of oxygen, nutrients, and waste products.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:MultiPromptChain] [24.04s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the functions of the heart?\",\n",
      "  \"text\": \"The heart is a vital organ that performs several important functions in the body. Some of the main functions of the heart include:\\n\\n1. Pumping blood: The heart acts as a muscular pump that continuously circulates blood throughout the body. It receives oxygenated blood from the lungs and pumps it to the rest of the body, delivering oxygen and nutrients to the cells and organs.\\n\\n2. Oxygenation: The heart receives deoxygenated blood from the body and pumps it to the lungs, where it picks up oxygen and gets rid of carbon dioxide. This oxygenation process ensures that the body's cells receive the necessary oxygen for their proper functioning.\\n\\n3. Regulation of blood pressure: The heart helps regulate blood pressure by adjusting the force and rate of its contractions. It pumps blood with enough force to maintain adequate blood flow and pressure throughout the body.\\n\\n4. Circulatory system support: The heart is the central organ of the circulatory system, which includes blood vessels, arteries, veins, and capillaries. It ensures the continuous flow of blood, allowing nutrients, hormones, and other substances to be transported to various parts of the body.\\n\\n5. Removal of waste products: The heart helps remove waste products, such as carbon dioxide and metabolic byproducts, from the body by pumping deoxygenated blood to the lungs for oxygenation and elimination of waste gases.\\n\\n6. Hormone transportation: The heart transports hormones and other signaling molecules throughout the body, helping to regulate various physiological processes.\\n\\nOverall, the heart plays a crucial role in maintaining the body's overall health and functioning by ensuring the continuous circulation of oxygen, nutrients, and waste products.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the functions of the heart?',\n",
       " 'text': \"The heart is a vital organ that performs several important functions in the body. Some of the main functions of the heart include:\\n\\n1. Pumping blood: The heart acts as a muscular pump that continuously circulates blood throughout the body. It receives oxygenated blood from the lungs and pumps it to the rest of the body, delivering oxygen and nutrients to the cells and organs.\\n\\n2. Oxygenation: The heart receives deoxygenated blood from the body and pumps it to the lungs, where it picks up oxygen and gets rid of carbon dioxide. This oxygenation process ensures that the body's cells receive the necessary oxygen for their proper functioning.\\n\\n3. Regulation of blood pressure: The heart helps regulate blood pressure by adjusting the force and rate of its contractions. It pumps blood with enough force to maintain adequate blood flow and pressure throughout the body.\\n\\n4. Circulatory system support: The heart is the central organ of the circulatory system, which includes blood vessels, arteries, veins, and capillaries. It ensures the continuous flow of blood, allowing nutrients, hormones, and other substances to be transported to various parts of the body.\\n\\n5. Removal of waste products: The heart helps remove waste products, such as carbon dioxide and metabolic byproducts, from the body by pumping deoxygenated blood to the lungs for oxygenation and elimination of waste gases.\\n\\n6. Hormone transportation: The heart transports hormones and other signaling molecules throughout the body, helping to regulate various physiological processes.\\n\\nOverall, the heart plays a crucial role in maintaining the body's overall health and functioning by ensuring the continuous circulation of oxygen, nutrients, and waste products.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_prompt_chain(\"What is the functions of the heart?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
