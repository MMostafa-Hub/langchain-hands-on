{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAIChat\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import (\n",
    "    ResponseSchema,\n",
    "    StructuredOutputParser,\n",
    "    PydanticOutputParser,\n",
    ")\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.globals import set_debug\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "set_debug(True)  # Enable debug mode\n",
    "load_dotenv()  # Load the variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use OpenAI's LLM Models to generate text, But you can use any LLM you want from the supported llms within langchain e.i. HuggingFace, LLaMa, PaLM, and etc.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically we will use the newer models that are connected to `v1/chat/completion` endpoint, and can be used in `OpenAIChat` class. \n",
    "\n",
    "for more information about the models and the endpoint please refer to [OpenAI's API Docs](https://platform.openai.com/docs/guides/text-generation/chat-completions-vs-completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To setup your OpenAI API key: \n",
    "1. Create an OpenAI account\n",
    "2. Create a new API key from this [link](https://platform.openai.com/api-keys)\n",
    "3. Copy the API key and paste it in the `OPENAI_API_KEY` variable in `.env` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmostafa/miniconda3/envs/aienv10/lib/python3.10/site-packages/langchain/llms/openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Temperature: a number between 0 and 1. The closer to 0,\\\n",
    "# the more predictable the text is, the closer to 1, the more Hallucinations.\n",
    "\n",
    "llm = OpenAIChat(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Hello, how are you?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAIChat] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Hello! I'm just a language model AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 13,\n",
      "      \"completion_tokens\": 32,\n",
      "      \"total_tokens\": 45\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To make sure the model is working, we can test it with a prompt.\n",
    "\n",
    "response = llm(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a language model AI, so I don't have feelings, but I'm here to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building apps you need the model to generate output in specific format each inference, so when building a prompt you need to follow some prompt engineering techniques, which are discussed in [Deeplearning.ai's ChatGPT Prompt Engineering for Developers](https://learn.deeplearning.ai/chatgpt-prompt-eng) go check it out it's free.\n",
    "\n",
    "I've created prompts that meets our needs in the examples that follows the guidelines provided in the short course, so don't worry about it being complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use f-strings to insert the text into the prompt.\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the text delimited by triple backticks \\ \n",
      "into a single sentence.\n",
      "```\n",
      "You should express what you want a model to do by \\ \n",
      "providing instructions that are as clear and \\ \n",
      "specific as you can possibly make them. \\ \n",
      "This will guide the model towards the desired output, \\ \n",
      "and reduce the chances of receiving irrelevant \\ \n",
      "or incorrect responses. Don't confuse writing a \\ \n",
      "clear prompt with writing a short prompt. \\ \n",
      "In many cases, longer prompts provide more clarity \\ \n",
      "and context for the model, which can lead to \\ \n",
      "more detailed and relevant outputs.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Summarize the text delimited by triple backticks \\\\ \\ninto a single sentence.\\n```\\nYou should express what you want a model to do by \\\\ \\nproviding instructions that are as clear and \\\\ \\nspecific as you can possibly make them. \\\\ \\nThis will guide the model towards the desired output, \\\\ \\nand reduce the chances of receiving irrelevant \\\\ \\nor incorrect responses. Don't confuse writing a \\\\ \\nclear prompt with writing a short prompt. \\\\ \\nIn many cases, longer prompts provide more clarity \\\\ \\nand context for the model, which can lead to \\\\ \\nmore detailed and relevant outputs.\\n```\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAIChat] [4.55s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Clear and specific instructions for a model will guide it towards the desired output and reduce the chances of irrelevant or incorrect responses, and longer prompts can provide more clarity and context for the model, leading to more detailed and relevant outputs.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 134,\n",
      "      \"completion_tokens\": 45,\n",
      "      \"total_tokens\": 179\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Getting the response from the model.\n",
    "response = llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts sometimes will be long and contains lots of parameters, so Langchain provides a simple way to generate prompts for your LLMs, you can use the `PromptTemplate` class to generate prompts for your LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text delimited by triple backticks, \\\n",
    "please extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python List.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "and the values for each key are the information extracted from the text.\n",
    "\n",
    "```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=review_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(text=customer_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text delimited by triple backticks, please extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python List.\n",
      "\n",
      "Format the output as JSON with the following keys:\n",
      "gift\n",
      "delivery_days\n",
      "price_value\n",
      "\n",
      "and the values for each key are the information extracted from the text.\n",
      "\n",
      "```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"For the following text delimited by triple backticks, please extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python List.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\nand the values for each key are the information extracted from the text.\\n\\n```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n```\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAIChat] [2.96s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"gift\\\": true,\\n  \\\"delivery_days\\\": 2,\\n  \\\"price_value\\\": [\\\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\\"]\\n}\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 239,\n",
      "      \"completion_tokens\": 48,\n",
      "      \"total_tokens\": 287\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output response from the LLMs is not always in the format you want. So Langchain provides a simple way to parse the output response from the LLMs, you can use different Parser classes to parse the output response based on your desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of the response without using any parser is a `str`, so we will use a parser to parse the response to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_resp = parser.parse(response)\n",
    "dict_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dict_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more sophisticated parsers that can add Schema, and Validation to the response, e.i. `StructuredOutputParser` and `PydanticOutputParser`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the output.\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"gift\",\n",
    "        type=\"bool\",\n",
    "        description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown\",\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"delivery_days\",\n",
    "        type=\"int\",\n",
    "        description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\",\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"price_value\",\n",
    "        type=\"list\",\n",
    "        description=\"Extract any sentences about the value or price, and output them as a comma separated Python List.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_parser = StructuredOutputParser(response_schemas=response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `StructuredOutputParser` and providing a `response_schema`, we can use the response schema in the prompt template to generate the prompt with the correct format, and the parser will parse the response to the provided schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of hardcoding the output schema, we can use different parsers to add it automatically.\n",
    "structured_review_template = \"\"\"\\\n",
    "The customer review for a product is delimited by triple backticks.\n",
    "```{text}```\n",
    "\n",
    "Extract the following information from the review, \\\n",
    "and output it in the specified format:\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt_template = PromptTemplate(\n",
    "    template=structured_review_template,\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": structured_parser.get_format_instructions()\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The customer review for a product is delimited by triple backticks.\n",
      "```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "```\n",
      "\n",
      "Extract the following information from the review, and output it in the specified format:\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": bool  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown\n",
      "\t\"delivery_days\": int  // How many days did it take for the product to arrive? If this information is not found, output -1.\n",
      "\t\"price_value\": list  // Extract any sentences about the value or price, and output them as a comma separated Python List.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structured_prompt = structured_prompt_template.format(text=customer_review)\n",
    "print(structured_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The customer review for a product is delimited by triple backticks.\\n```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n```\\n\\nExtract the following information from the review, and output it in the specified format:\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \\\"```json\\\" and \\\"```\\\":\\n\\n```json\\n{\\n\\t\\\"gift\\\": bool  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown\\n\\t\\\"delivery_days\\\": int  // How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\t\\\"price_value\\\": list  // Extract any sentences about the value or price, and output them as a comma separated Python List.\\n}\\n```\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAIChat] [46.91s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n\\t\\\"gift\\\": true,\\n\\t\\\"delivery_days\\\": 2,\\n\\t\\\"price_value\\\": [\\\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\\"]\\n}\\n```\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 270,\n",
      "      \"completion_tokens\": 52,\n",
      "      \"total_tokens\": 322\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "structured_response = llm(structured_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(structured_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_parser.parse(structured_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PydanticOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review(BaseModel):\n",
    "    gift: bool = Field(\n",
    "        description=\"Was the item purchased as a gift for someone else?, True if yes, False if not or unknown.\",\n",
    "        default=False,\n",
    "    )\n",
    "    delivery_days: int = Field(\n",
    "        description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\",\n",
    "        default=-1,\n",
    "        gt=-1,\n",
    "    )\n",
    "    price_value: List[str] = Field(\n",
    "        description=\"Extract any sentences about the value or price, and output them as a comma separated Python List.\",\n",
    "        default=[],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydantic_parser = PydanticOutputParser(pydantic_object=Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydantic_prompt_template = PromptTemplate(\n",
    "    template=structured_review_template,\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": pydantic_parser.get_format_instructions()\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The customer review for a product is delimited by triple backticks.\n",
      "```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "```\n",
      "\n",
      "Extract the following information from the review, and output it in the specified format:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"gift\": {\"title\": \"Gift\", \"description\": \"Was the item purchased as a gift for someone else?, True if yes, False if not or unknown.\", \"default\": false, \"type\": \"boolean\"}, \"delivery_days\": {\"title\": \"Delivery Days\", \"description\": \"How many days did it take for the product to arrive? If this information is not found, output -1.\", \"default\": -1, \"exclusiveMinimum\": -1, \"type\": \"integer\"}, \"price_value\": {\"title\": \"Price Value\", \"description\": \"Extract any sentences about the value or price, and output them as a comma separated Python List.\", \"default\": [], \"type\": \"array\", \"items\": {\"type\": \"string\"}}}}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pydantic_prompt = pydantic_prompt_template.format(text=customer_review)\n",
    "print(pydantic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The customer review for a product is delimited by triple backticks.\\n```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n```\\n\\nExtract the following information from the review, and output it in the specified format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"gift\\\": {\\\"title\\\": \\\"Gift\\\", \\\"description\\\": \\\"Was the item purchased as a gift for someone else?, True if yes, False if not or unknown.\\\", \\\"default\\\": false, \\\"type\\\": \\\"boolean\\\"}, \\\"delivery_days\\\": {\\\"title\\\": \\\"Delivery Days\\\", \\\"description\\\": \\\"How many days did it take for the product to arrive? If this information is not found, output -1.\\\", \\\"default\\\": -1, \\\"exclusiveMinimum\\\": -1, \\\"type\\\": \\\"integer\\\"}, \\\"price_value\\\": {\\\"title\\\": \\\"Price Value\\\", \\\"description\\\": \\\"Extract any sentences about the value or price, and output them as a comma separated Python List.\\\", \\\"default\\\": [], \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}}\\n```\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:OpenAIChat] [3.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"gift\\\": true,\\n  \\\"delivery_days\\\": 2,\\n  \\\"price_value\\\": [\\\"It arrived in two days, just in time for my wife's anniversary present.\\\", \\\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\\"]\\n}\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 427,\n",
      "      \"completion_tokens\": 65,\n",
      "      \"total_tokens\": 492\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pydantic_response = llm(pydantic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It arrived in two days, just in time for my wife's anniversary present.\", \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(pydantic_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review(gift=True, delivery_days=2, price_value=[\"It arrived in two days, just in time for my wife's anniversary present.\", \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a Review object.\n",
    "pydantic_parser.parse(pydantic_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect the Prompt Template, Model, and the Parser together in one object called a `Chain`, and then use it to generate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=structured_prompt_template,\n",
    "    output_parser=structured_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The customer review for a product is delimited by triple backticks.\\n```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n```\\n\\nExtract the following information from the review, and output it in the specified format:\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \\\"```json\\\" and \\\"```\\\":\\n\\n```json\\n{\\n\\t\\\"gift\\\": bool  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown\\n\\t\\\"delivery_days\\\": int  // How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\t\\\"price_value\\\": list  // Extract any sentences about the value or price, and output them as a comma separated Python List.\\n}\\n```\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OpenAIChat] [2.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n\\t\\\"gift\\\": true,\\n\\t\\\"delivery_days\\\": 2,\\n\\t\\\"price_value\\\": [\\\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\\"]\\n}\\n```\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 270,\n",
      "      \"completion_tokens\": 52,\n",
      "      \"total_tokens\": 322\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [2.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": {\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\n",
      "      \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': {'gift': True,\n",
       "  'delivery_days': 2,\n",
       "  'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_chain({\"text\": customer_review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydantic_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=pydantic_prompt_template,\n",
    "    output_parser=pydantic_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OpenAIChat] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"The customer review for a product is delimited by triple backticks.\\n```This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n```\\n\\nExtract the following information from the review, and output it in the specified format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"gift\\\": {\\\"title\\\": \\\"Gift\\\", \\\"description\\\": \\\"Was the item purchased as a gift for someone else?, True if yes, False if not or unknown.\\\", \\\"default\\\": false, \\\"type\\\": \\\"boolean\\\"}, \\\"delivery_days\\\": {\\\"title\\\": \\\"Delivery Days\\\", \\\"description\\\": \\\"How many days did it take for the product to arrive? If this information is not found, output -1.\\\", \\\"default\\\": -1, \\\"exclusiveMinimum\\\": -1, \\\"type\\\": \\\"integer\\\"}, \\\"price_value\\\": {\\\"title\\\": \\\"Price Value\\\", \\\"description\\\": \\\"Extract any sentences about the value or price, and output them as a comma separated Python List.\\\", \\\"default\\\": [], \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}}\\n```\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo-1106 in organization org-dnanFMzkqEPXh6Dgutwmyuw0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OpenAIChat] [24.39s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"gift\\\": false,\\n  \\\"delivery_days\\\": 2,\\n  \\\"price_value\\\": [\\\"It arrived in two days, just in time for my wife's anniversary present.\\\", \\\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\\"]\\n}\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 427,\n",
      "      \"completion_tokens\": 65,\n",
      "      \"total_tokens\": 492\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [24.40s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': Review(gift=False, delivery_days=2, price_value=[\"It arrived in two days, just in time for my wife's anniversary present.\", \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydantic_chain({\"text\": customer_review})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
